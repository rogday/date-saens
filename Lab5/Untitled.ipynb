{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Epoch:  1\n",
      "Epoch:  2\n",
      "Epoch:  3\n",
      "Epoch:  4\n",
      "Epoch:  5\n",
      "Epoch:  6\n",
      "Epoch:  7\n",
      "Epoch:  8\n",
      "Epoch:  9\n",
      "Accuracy 0.89246154\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv', header = None)\n",
    "\n",
    "y_train = train.iloc[:, 0:1].copy()\n",
    "x_train = train.iloc[:, 1:785].copy()\n",
    "\n",
    "test = pd.read_csv('test.csv', header = None)\n",
    "\n",
    "y_test = test.iloc[:, 0:1].copy()\n",
    "x_test = test.iloc[:, 1:785].copy()\n",
    "\n",
    "#---------------------------------------\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "x_train = x_train.astype('float32')/255.\n",
    "x_test = x_test.astype('float32')/255.\n",
    "\n",
    "x_train = x_train.values.reshape((-1, 784))\n",
    "x_test = x_test.values.reshape((-1, 784))\n",
    "\n",
    "N, M = x_train.shape\n",
    "O = 1024\n",
    "P = 256\n",
    "T = 27\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "\n",
    "inp = tf.placeholder(shape=(None, M), dtype=tf.float32)\n",
    "out = tf.placeholder(shape=(None, T), dtype=tf.float32)\n",
    "\n",
    "weights1 = tf.Variable(tf.truncated_normal([M, O], stddev=np.sqrt(1/M)))\n",
    "bias1 = tf.Variable(tf.ones([O])/O)\n",
    "layer1 = tf.nn.relu(tf.matmul(inp, weights1) + bias1)\n",
    "\n",
    "weights2 = tf.Variable(tf.truncated_normal([O, P], stddev=np.sqrt(1/O)))\n",
    "bias2 = tf.Variable(tf.ones([P])/P)\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, weights2) + bias2)\n",
    "\n",
    "weights_output = tf.Variable(tf.truncated_normal([P, T], stddev=np.sqrt(1/P)))\n",
    "bias_output = tf.Variable(tf.ones([T])/T)\n",
    "evidence = tf.matmul(layer2, weights_output) + bias_output\n",
    "output = tf.nn.softmax(evidence)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=evidence, labels= out)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(out, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.scalar(\"Cost\", cross_entropy)\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(training_epochs):\n",
    "            print(\"Epoch: \", epoch)\n",
    "            batch_count = int(N/batch_size)\n",
    "            for i in range(batch_count):\n",
    "                start, finish = batch_size*i, batch_size*(i+1)\n",
    "                batch_x, batch_y = x_train[start:finish, :], y_train[start:finish, :]\n",
    "                d = {inp: batch_x, out: batch_y}\n",
    "                _, summary = sess.run([train_step, summary_op], feed_dict=d)\n",
    "        print(\"Accuracy\", accuracy.eval(feed_dict={inp: x_test, out: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "65000/65000 [==============================] - 20s 309us/step - loss: 0.9816 - acc: 0.7166\n",
      "Epoch 2/10\n",
      "65000/65000 [==============================] - 16s 240us/step - loss: 0.5630 - acc: 0.8308\n",
      "Epoch 3/10\n",
      "65000/65000 [==============================] - 16s 252us/step - loss: 0.4441 - acc: 0.8638\n",
      "Epoch 4/10\n",
      "65000/65000 [==============================] - 19s 296us/step - loss: 0.3814 - acc: 0.8804\n",
      "Epoch 5/10\n",
      "65000/65000 [==============================] - 20s 307us/step - loss: 0.3416 - acc: 0.8931\n",
      "Epoch 6/10\n",
      "65000/65000 [==============================] - 23s 359us/step - loss: 0.3138 - acc: 0.8997\n",
      "Epoch 7/10\n",
      "65000/65000 [==============================] - 25s 390us/step - loss: 0.2912 - acc: 0.9063\n",
      "Epoch 8/10\n",
      "65000/65000 [==============================] - 26s 404us/step - loss: 0.2739 - acc: 0.9123\n",
      "Epoch 9/10\n",
      "65000/65000 [==============================] - 28s 438us/step - loss: 0.2572 - acc: 0.9171\n",
      "Epoch 10/10\n",
      "65000/65000 [==============================] - 31s 469us/step - loss: 0.2435 - acc: 0.9190\n",
      "13000/13000 [==============================] - 4s 286us/step\n",
      "[0.3986142090879954, 0.8815384603463686]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv', header = None)\n",
    "\n",
    "y_train = train.iloc[:, 0:1].copy()\n",
    "x_train = train.iloc[:, 1:785].copy()\n",
    "\n",
    "test = pd.read_csv('test.csv', header = None)\n",
    "\n",
    "y_test = test.iloc[:, 0:1].copy()\n",
    "x_test = test.iloc[:, 1:785].copy()\n",
    "\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "x_train = x_train.astype('float32')/255.\n",
    "x_test = x_test.astype('float32')/255.\n",
    "\n",
    "x_train = x_train.values.reshape((-1, 784))\n",
    "x_test = x_test.values.reshape((-1, 784))\n",
    "\n",
    "N, M = x_train.shape\n",
    "O = 1024\n",
    "P = 256\n",
    "T = 27\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(P, activation='relu'))\n",
    "model.add(Dense(T, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=training_epochs, batch_size=batch_size)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "65000/65000 [==============================] - 24s 370us/step - loss: 0.9164 - acc: 0.7301\n",
      "Epoch 2/10\n",
      "65000/65000 [==============================] - 23s 355us/step - loss: 0.5151 - acc: 0.8427\n",
      "Epoch 3/10\n",
      "65000/65000 [==============================] - 26s 407us/step - loss: 0.4175 - acc: 0.8714\n",
      "Epoch 4/10\n",
      "65000/65000 [==============================] - 30s 458us/step - loss: 0.3707 - acc: 0.8835\n",
      "Epoch 5/10\n",
      "65000/65000 [==============================] - 33s 505us/step - loss: 0.3399 - acc: 0.8933\n",
      "Epoch 6/10\n",
      "65000/65000 [==============================] - 35s 531us/step - loss: 0.3155 - acc: 0.8994\n",
      "Epoch 7/10\n",
      "65000/65000 [==============================] - 38s 581us/step - loss: 0.2935 - acc: 0.9065\n",
      "Epoch 8/10\n",
      "65000/65000 [==============================] - 87s 1ms/step - loss: 0.2759 - acc: 0.9112\n",
      "Epoch 9/10\n",
      "65000/65000 [==============================] - 75s 1ms/step - loss: 0.2537 - acc: 0.9172\n",
      "Epoch 10/10\n",
      "65000/65000 [==============================] - 42s 640us/step - loss: 0.2408 - acc: 0.9206\n",
      "13000/13000 [==============================] - 5s 361us/step\n",
      "[0.4834602659711471, 0.8752307658012096]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv', header = None)\n",
    "\n",
    "y_train = train.iloc[:, 0:1].copy()\n",
    "x_train = train.iloc[:, 1:785].copy()\n",
    "\n",
    "test = pd.read_csv('test.csv', header = None)\n",
    "\n",
    "y_test = test.iloc[:, 0:1].copy()\n",
    "x_test = test.iloc[:, 1:785].copy()\n",
    "\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "x_train = x_train.astype('float32')/255.\n",
    "x_test = x_test.astype('float32')/255.\n",
    "\n",
    "x_train = x_train.values.reshape((-1, 784))\n",
    "x_test = x_test.values.reshape((-1, 784))\n",
    "\n",
    "N, M = x_train.shape\n",
    "O = 2048\n",
    "P = 512\n",
    "T = 27\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(P, activation='relu'))\n",
    "model.add(Dense(T, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=training_epochs, batch_size=batch_size)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "65000/65000 [==============================] - 31s 483us/step - loss: 0.9184 - acc: 0.7272\n",
      "Epoch 2/10\n",
      "65000/65000 [==============================] - 23s 353us/step - loss: 0.5172 - acc: 0.8427\n",
      "Epoch 3/10\n",
      "65000/65000 [==============================] - 24s 373us/step - loss: 0.4217 - acc: 0.8694\n",
      "Epoch 4/10\n",
      "65000/65000 [==============================] - 27s 409us/step - loss: 0.3735 - acc: 0.8826\n",
      "Epoch 5/10\n",
      "65000/65000 [==============================] - 30s 456us/step - loss: 0.3430 - acc: 0.8913\n",
      "Epoch 6/10\n",
      "65000/65000 [==============================] - 30s 460us/step - loss: 0.3253 - acc: 0.8972\n",
      "Epoch 7/10\n",
      "65000/65000 [==============================] - 28s 437us/step - loss: 0.3081 - acc: 0.9028\n",
      "Epoch 8/10\n",
      "65000/65000 [==============================] - 31s 484us/step - loss: 0.2924 - acc: 0.9072\n",
      "Epoch 9/10\n",
      "65000/65000 [==============================] - 30s 467us/step - loss: 0.2790 - acc: 0.9111\n",
      "Epoch 10/10\n",
      "65000/65000 [==============================] - 32s 489us/step - loss: 0.2795 - acc: 0.9115\n",
      "13000/13000 [==============================] - 3s 240us/step\n",
      "[0.49944013104988977, 0.8743846136790056]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"211pt\" viewBox=\"0.00 0.00 248.00 211.00\" width=\"248pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 207)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-207 244,-207 244,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139713384686536 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139713384686536</title>\n",
       "<polygon fill=\"none\" points=\"0,-83.5 0,-129.5 240,-129.5 240,-83.5 0,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"51\" y=\"-102.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"102,-83.5 102,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"129.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"102,-106.5 157,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"129.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"157,-83.5 157,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-114.3\">(None, 784)</text>\n",
       "<polyline fill=\"none\" points=\"157,-106.5 240,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-91.3\">(None, 512)</text>\n",
       "</g>\n",
       "<!-- 139714492286680 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139714492286680</title>\n",
       "<polygon fill=\"none\" points=\"0,-.5 0,-46.5 240,-46.5 240,-.5 0,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"51\" y=\"-19.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"102,-.5 102,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"129.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"102,-23.5 157,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"129.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"157,-.5 157,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-31.3\">(None, 512)</text>\n",
       "<polyline fill=\"none\" points=\"157,-23.5 240,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198.5\" y=\"-8.3\">(None, 27)</text>\n",
       "</g>\n",
       "<!-- 139713384686536&#45;&gt;139714492286680 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139713384686536-&gt;139714492286680</title>\n",
       "<path d=\"M120,-83.3799C120,-75.1745 120,-65.7679 120,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"123.5001,-56.784 120,-46.784 116.5001,-56.784 123.5001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139713378779712 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139713378779712</title>\n",
       "<polygon fill=\"none\" points=\"61,-166.5 61,-202.5 179,-202.5 179,-166.5 61,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120\" y=\"-180.8\">139713378779712</text>\n",
       "</g>\n",
       "<!-- 139713378779712&#45;&gt;139713384686536 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139713378779712-&gt;139713384686536</title>\n",
       "<path d=\"M120,-166.4092C120,-158.4308 120,-148.795 120,-139.606\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"123.5001,-139.5333 120,-129.5333 116.5001,-139.5334 123.5001,-139.5333\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv', header = None)\n",
    "\n",
    "y_train = train.iloc[:, 0:1].copy()\n",
    "x_train = train.iloc[:, 1:785].copy()\n",
    "\n",
    "test = pd.read_csv('test.csv', header = None)\n",
    "\n",
    "y_test = test.iloc[:, 0:1].copy()\n",
    "x_test = test.iloc[:, 1:785].copy()\n",
    "\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "x_train = x_train.astype('float32')/255.\n",
    "x_test = x_test.astype('float32')/255.\n",
    "\n",
    "x_train = x_train.values.reshape((-1, 784))\n",
    "x_test = x_test.values.reshape((-1, 784))\n",
    "\n",
    "N, M = x_train.shape\n",
    "O = 1024\n",
    "P = 512\n",
    "T = 27\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(P, activation='relu'))\n",
    "model.add(Dense(T, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=training_epochs, batch_size=batch_size)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(score)\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes = True).create(prog = 'dot', format = 'svg'))\n",
    "#пакеты: graphviz, pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "65000/65000 [==============================] - 25s 383us/step - loss: 0.8939 - acc: 0.7385\n",
      "Epoch 2/10\n",
      "65000/65000 [==============================] - 23s 351us/step - loss: 0.4905 - acc: 0.8499\n",
      "Epoch 3/10\n",
      "65000/65000 [==============================] - 25s 379us/step - loss: 0.3852 - acc: 0.8797\n",
      "Epoch 4/10\n",
      "65000/65000 [==============================] - 25s 390us/step - loss: 0.3301 - acc: 0.8961\n",
      "Epoch 5/10\n",
      "65000/65000 [==============================] - 28s 429us/step - loss: 0.2941 - acc: 0.9059\n",
      "Epoch 6/10\n",
      "65000/65000 [==============================] - 30s 455us/step - loss: 0.2642 - acc: 0.9136\n",
      "Epoch 7/10\n",
      "65000/65000 [==============================] - 32s 490us/step - loss: 0.2404 - acc: 0.9214\n",
      "Epoch 8/10\n",
      "65000/65000 [==============================] - 34s 531us/step - loss: 0.2200 - acc: 0.9260\n",
      "Epoch 9/10\n",
      "65000/65000 [==============================] - 37s 562us/step - loss: 0.2034 - acc: 0.9313\n",
      "Epoch 10/10\n",
      "65000/65000 [==============================] - 38s 587us/step - loss: 0.1863 - acc: 0.9355\n",
      "13000/13000 [==============================] - 4s 275us/step\n",
      "[0.42845061352619757, 0.8826153856057387]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv', header = None)\n",
    "\n",
    "y_train = train.iloc[:, 0:1].copy()\n",
    "x_train = train.iloc[:, 1:785].copy()\n",
    "\n",
    "test = pd.read_csv('test.csv', header = None)\n",
    "\n",
    "y_test = test.iloc[:, 0:1].copy()\n",
    "x_test = test.iloc[:, 1:785].copy()\n",
    "\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "x_train = x_train.astype('float32')/255.\n",
    "x_test = x_test.astype('float32')/255.\n",
    "\n",
    "x_train = x_train.values.reshape((-1, 784))\n",
    "x_test = x_test.values.reshape((-1, 784))\n",
    "\n",
    "N, M = x_train.shape\n",
    "O = 2048\n",
    "P = 512\n",
    "T = 27\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "early_stopping = EarlyStopping(monitor = 'loss')\n",
    "tensorboard = TensorBoard(log_dir = './logs', write_graph = True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(P, activation='relu'))\n",
    "model.add(Dense(T, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=training_epochs, batch_size=batch_size, callbacks = [early_stopping, tensorboard])\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.12.0 at http://localhost.localdomain:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "65000/65000 [==============================] - 74s 1ms/step - loss: 0.8602 - acc: 0.7587\n",
      "Epoch 2/10\n",
      "65000/65000 [==============================] - 80s 1ms/step - loss: 4.8178 - acc: 0.6276\n",
      "13000/13000 [==============================] - 8s 620us/step\n",
      "[10.334112064655011, 0.34884615586354184]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv', header = None)\n",
    "\n",
    "y_train = train.iloc[:, 0:1].copy()\n",
    "x_train = train.iloc[:, 1:785].copy()\n",
    "\n",
    "test = pd.read_csv('test.csv', header = None)\n",
    "\n",
    "y_test = test.iloc[:, 0:1].copy()\n",
    "x_test = test.iloc[:, 1:785].copy()\n",
    "\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "x_train = x_train.astype('float32')/255.\n",
    "x_test = x_test.astype('float32')/255.\n",
    "\n",
    "x_train = x_train.values.reshape((-1, 784))\n",
    "x_test = x_test.values.reshape((-1, 784))\n",
    "\n",
    "N, M = x_train.shape\n",
    "O = 2048\n",
    "O1 = 1024\n",
    "P = 512\n",
    "P1 = 256\n",
    "T = 27\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "early_stopping = EarlyStopping(monitor = 'loss')\n",
    "tensorboard = TensorBoard(log_dir = './logs', write_graph = True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(O1, activation='relu'))\n",
    "model.add(Dense(P, activation='relu'))\n",
    "model.add(Dense(P1, activation='relu'))\n",
    "model.add(Dense(T, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=training_epochs, batch_size=batch_size, callbacks = [early_stopping, tensorboard])\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
